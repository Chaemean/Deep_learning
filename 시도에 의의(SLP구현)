import numpy as np
import matplotlib.pyplot as plt

class MLP():
    def __init__(self, func, gradient, tol=1e-3,  lr=0.2, epochs=60000):
        self.func = func
        self.gradient = gradient  #relu 함수는 성능이 너무 안 좋다..
        self.tol = tol
        self.lr = lr             #0.1~0.3 정도가 가장 성능이 좋다
        self.epochs = epochs

    def actf(self, x):
        if self.func=='sigmoid': return 1 / (1 + np.exp(-x))
        elif self.func=='relu' : return np.maximum(x, 0)
        elif self.func=='tanh' : return np.tanh(x)
        else : print("잘못된 활성화 함수입니다.")

    def act_deriv(self, x):
        if self.func=='sigmoid': return x * (1 - x)
        elif self.func=='relu' : return np.where(x > self.tol, 1, 0)
        elif self.func=='tanh' : return 1-x*x
        else : print("잘못된 활성화 함수입니다.")

    def predict(self, X):

        layer0 = X  # 입력을 layer0에 대입한다.
        S1 = np.dot(layer0, self.W1) + self.B1  # 행렬의 곱을 계산한다.
        layer1 = self.actf(S1)  # 활성화 함수를 적용한다.
        S2 = np.dot(layer1, self.W2) + self.B2  # 행렬의 곱을 계산한다.
        layer2 = self.actf(S2)  # 활성화 함수를 적용한다.

        return layer0, layer1, layer2  # 입력값, h값, 출력값


    def fit(self, X, T, batch_size=1):
        X_rows, X_cols = X.shape
        inputs, hiddens, outputs = X_cols, X_cols, 1

        self.batch_size = batch_size

        if self.gradient == 'SGD': self.batch_size=1
        elif self.gradient == 'GD': self.batch_size=X_rows

        # 가중치를 –1.0에서 1.0 사이의 난수로 초기화
        self.W1 = 2 * np.random.random((inputs, hiddens)) - 1
        self.W2 = 2 * np.random.random((hiddens, outputs)) - 1
        self.B1 = 2 * np.random.random(hiddens) - 1
        self.B2 = 2 * np.random.random(outputs) - 1

        error_history = []

        for i in range(self.epochs):
            # 데이터셋의 길이만큼 인덱스 배열 생성
            indices = np.arange(X.shape[0])

            # 인덱스 배열 랜덤하게 섞기
            np.random.shuffle(indices)

            # 인덱스를 사용하여 데이터셋과 레이블 섞기
            X_shuffled = X[indices]
            y_shuffled = T[indices]

            error = 0

            for start_idx in range (0, X_shuffled.shape[0], self.batch_size):
                X_batch = X_shuffled[start_idx:start_idx + self.batch_size] #배치 사이즈 만큼 자름
                y_batch = y_shuffled[start_idx:start_idx + self.batch_size]

                if len(X_batch) < self.batch_size:
                    break

                layer0, layer1, layer2 = self.predict(X_batch)  # X 전체를 한번에 처리
                layer2_error = layer2 - y_batch
                layer2_delta = layer2_error * self.act_deriv(layer2)
                layer1_error = np.dot(layer2_delta, self.W2.T)
                layer1_delta = layer1_error * self.act_deriv(layer1)

                # 배치 개수인 batch_size로 나눠줌
                self.W2 += -self.lr * np.dot(layer1.T, layer2_delta) / self.batch_size
                self.W1 += -self.lr * np.dot(layer0.T, layer1_delta) / self.batch_size
                self.B2 += -self.lr * np.sum(layer2_delta, axis=0) / self.batch_size
                self.B1 += -self.lr * np.sum(layer1_delta, axis=0) / self.batch_size

            error += np.mean(np.abs(layer2_error))
            error_history.append(error / len(X))

        return error_history

    def pred(self, X):  #예측값 프린트
        X = np.reshape(X, (1, -1))

        layer0 = X  # 입력을 layer0에 대입한다.
        S1 = np.dot(layer0, self.W1) + self.B1  # 행렬의 곱을 계산한다.
        layer1 = self.actf(S1)  # 활성화 함수를 적용한다.
        S2 = np.dot(layer1, self.W2) + self.B2  # 행렬의 곱을 계산한다.
        layer2 = self.actf(S2)  # 활성화 함수를 적용한다.

        print(X, layer2)

    # 오차를 그래프로 표현하는 함수
    def plot_error_history(self, error_history):
        plt.figure(figsize=(10, 5))
        plt.plot(error_history)
        plt.xlabel('Epochs')
        plt.ylabel('Error')
        plt.title('MGD')
        plt.grid(True)
        plt.show()

#-------실습 적용-----------------------------------------------------------------------------------

# XOR 훈련 샘플과 정답
X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0],  [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])
T = np.array([[0], [1], [1], [0], [1], [0], [0], [1]])

mlp = MLP('tanh', 'MGD', epochs=100)
error_history = mlp.fit(X, T, batch_size=4) #SGD나 GD로 설정된 경우 배치사이즈 입력에 관계없이 자동 설정
mlp.pred([0,1,1])
mlp.plot_error_history(error_history)





